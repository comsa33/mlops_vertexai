{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"copyright"},"outputs":[],"source":["# Copyright 2024 Forusone(shins777@gmail.com)\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."]},{"cell_type":"markdown","metadata":{"id":"title:generic,gcp"},"source":["# Data preprocessing with diverse dataset types\n"]},{"cell_type":"markdown","metadata":{"id":"overview:mlops"},"source":["## Overview\n","This notebook simplifies [get_started_bq_datasets](https://colab.sandbox.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/datasets/get_started_bq_datasets.ipynb) in Google manual site. Use the original notebook to get more detailed information about this process. Learn more about [BigQuery datasets](https://cloud.google.com/bigquery/docs/datasets-intro) and [Vertex AI for BigQuery users](https://cloud.google.com/vertex-ai/docs/beginner/bqml).\n","\n","### Dataset\n","\n","* The dataset used for this tutorial is the GSOD dataset from [BigQuery public datasets](https://cloud.google.com/bigquery/public-data). In this version of the dataset you consider the fields year, month and day to predict the value of mean daily temperature (mean_temp)."]},{"cell_type":"markdown","metadata":{"id":"e5d353aa47ac"},"source":["## Install Vertex AI SDK"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9bbcbe73e685"},"outputs":[],"source":["! pip install --upgrade --quiet google-cloud-aiplatform \\\n","                                 google-cloud-bigquery \\\n","                                 tensorflow \\\n","                                 tensorflow-io \\\n","                                 xgboost \\\n","                                 numpy \\\n","                                 pandas \\\n","                                 pyarrow \\\n","                                 db-dtypes"]},{"cell_type":"markdown","metadata":{"id":"5dccb1c8feb6"},"source":["## Configuration\n","\n","### Authenticate your notebook environment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cc7251520a07","executionInfo":{"status":"ok","timestamp":1733620440777,"user_tz":-540,"elapsed":7617,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6a1e2502-4324-4760-f093-edbe255e2e80"},"outputs":[{"output_type":"stream","name":"stdout","text":["Updated property [core/project].\n"]}],"source":["import sys\n","from IPython.display import Markdown, display\n","\n","PROJECT_ID=\"ai-hangsik\"\n","LOCATION=\"us-central1\"\n","\n","# For only colab user, no need this process for Colab Enterprise in Vertex AI.\n","if \"google.colab\" in sys.modules:\n","    from google.colab import auth\n","    auth.authenticate_user(project_id=PROJECT_ID)\n","\n","# set project.\n","!gcloud config set project {PROJECT_ID}"]},{"cell_type":"markdown","source":["### Initialize Vertex AI SDK"],"metadata":{"id":"5g3VicK5juoa"}},{"cell_type":"code","source":["from google.cloud import aiplatform, bigquery\n","aiplatform.init(project=PROJECT_ID, location=LOCATION)"],"metadata":{"id":"G0822hg3juB7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bucket:mbsdk"},"source":["### Create a Cloud Storage bucket"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bucket","executionInfo":{"status":"ok","timestamp":1733620519578,"user_tz":-540,"elapsed":3807,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d3c0a0d3-be0f-46d6-9655-956f1c1358e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Creating gs://mlops-ai-hangsik-1207/...\n","ServiceException: 409 A Cloud Storage bucket named 'mlops-ai-hangsik-1207' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"]}],"source":["# Create a bucket.\n","BUCKET_URI = f\"gs://mlops-{PROJECT_ID}-1207\"\n","! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"]},{"cell_type":"markdown","metadata":{"id":"setup_vars"},"source":["### Import libraries and define constants"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"import_aip:mbsdk"},"outputs":[],"source":["import pandas as pd\n","import xgboost as xgb\n","from google.cloud import bigquery"]},{"cell_type":"markdown","source":["## Create DataSet from BigQuery"],"metadata":{"id":"vvvZXSHIm-Zz"}},{"cell_type":"markdown","metadata":{"id":"init_bq"},"source":["### Create BigQuery client\n"]},{"cell_type":"code","source":["IMPORT_FILE = \"bq://bigquery-public-data.samples.gsod\"\n","BQ_TABLE = \"bigquery-public-data.samples.gsod\""],"metadata":{"id":"Ir2hZkCY1PAu"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cY3SPgFalqKy"},"outputs":[],"source":["bqclient = bigquery.Client(project=PROJECT_ID)"]},{"cell_type":"markdown","metadata":{"id":"create_dataset:tabular,bq,lrg"},"source":["### Create the dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o5S6ssH-lqKz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733617231572,"user_tz":-540,"elapsed":3455,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"f36f4ec6-c7ab-4b14-9748-f93d1e4924ea"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:google.cloud.aiplatform.datasets.dataset:Creating TabularDataset\n","INFO:google.cloud.aiplatform.datasets.dataset:Create TabularDataset backing LRO: projects/721521243942/locations/us-central1/datasets/7610287873593442304/operations/2263486969453477888\n","INFO:google.cloud.aiplatform.datasets.dataset:TabularDataset created. Resource name: projects/721521243942/locations/us-central1/datasets/7610287873593442304\n","INFO:google.cloud.aiplatform.datasets.dataset:To use this TabularDataset in another session:\n","INFO:google.cloud.aiplatform.datasets.dataset:ds = aiplatform.TabularDataset('projects/721521243942/locations/us-central1/datasets/7610287873593442304')\n"]},{"output_type":"stream","name":"stdout","text":["projects/721521243942/locations/us-central1/datasets/7610287873593442304\n"]}],"source":["# https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.TabularDataset\n","# https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.TabularDataset#google_cloud_aiplatform_TabularDataset_create\n","\n","dataset = aiplatform.TabularDataset.create(\n","    display_name=\"NOAA historical weather data - bq dataset\",\n","    bq_source=[IMPORT_FILE],\n","    labels={\"user_metadata\": BUCKET_URI[5:]},\n",")\n","\n","label_column = \"mean_temp\"\n","\n","print(dataset.resource_name)"]},{"cell_type":"code","source":["print(type(dataset))"],"metadata":{"id":"lvlL-K3fnGYB","executionInfo":{"status":"ok","timestamp":1733616551806,"user_tz":-540,"elapsed":309,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"ad2e6021-18e2-480a-eddf-b290abc6fffd","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'google.cloud.aiplatform.datasets.tabular_dataset.TabularDataset'>\n"]}]},{"cell_type":"markdown","source":["## Create DataSet from GCS(Google Cloud Storage)"],"metadata":{"id":"WHD-W01QnHsj"}},{"cell_type":"markdown","metadata":{"id":"bq_extract"},"source":["### Copy the dataset to Cloud Storage\n","\n","make a copy of the BigQuery table as a CSV file, to Cloud Storage using the BigQuery extract command.[BigQuery command line interface](https://cloud.google.com/bigquery/docs/reference/bq-cli-reference)."]},{"cell_type":"markdown","source":["#### Create CSV files into GCS"],"metadata":{"id":"jmfGlcTQrd0f"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ioNTuGMBlqKz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733617017447,"user_tz":-540,"elapsed":79215,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"a84212cb-29f5-47a2-af86-68048fff7fe7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Waiting on bqjob_r7685f3a55d0c4eed_00000193a39f3684_1 ... (66s) Current status: DONE   \n","['gs://mlops-ai-hangsik-1207/mydata000000000097.csv', 'gs://mlops-ai-hangsik-1207/mydata000000000098.csv', 'gs://mlops-ai-hangsik-1207/mydata000000000099.csv', 'gs://mlops-ai-hangsik-1207/mydata000000000100.csv', 'gs://mlops-ai-hangsik-1207/mydata000000000101.csv', 'gs://mlops-ai-hangsik-1207/mydata000000000102.csv', 'gs://mlops-ai-hangsik-1207/mydata000000000103.csv', 'gs://mlops-ai-hangsik-1207/mydata000000000104.csv', 'gs://mlops-ai-hangsik-1207/mydata000000000105.csv', 'gs://mlops-ai-hangsik-1207/mydata000000000106.csv', 'gs://mlops-ai-hangsik-1207/mydata000000000107.csv', 'gs://mlops-ai-hangsik-1207/mydata000000000108.csv', 'gs://mlops-ai-hangsik-1207/mydata000000000109.csv', 'gs://mlops-ai-hangsik-1207/mydata000000000110.csv', 'gs://mlops-ai-hangsik-1207/mydata000000000111.csv', 'gs://mlops-ai-hangsik-1207/mydata000000000112.csv', 'gs://mlops-ai-hangsik-1207/mydata000000000113.csv', 'gs://mlops-ai-hangsik-1207/mydata000000000114.csv']\n","station_number,wban_number,year,month,day,mean_temp,num_mean_temp_samples,mean_dew_point,num_mean_dew_point_samples,mean_sealevel_pressure,num_mean_sealevel_pressure_samples,mean_station_pressure,num_mean_station_pressure_samples,mean_visibility,num_mean_visibility_samples,mean_wind_speed,num_mean_wind_speed_samples,max_sustained_wind_speed,max_gust_wind_speed,max_temperature,max_temperature_explicit,min_temperature,min_temperature_explicit,total_precipitation,snow_depth,fog,rain,snow,hail,thunder,tornado\n","33960,99999,1929,11,21,44.700000762939453,4,43.299999237060547,4,1004.2000122070312,4,,,2.5,4,10,4,13,,42.099998474121094,false,,,0,,false,false,false,false,false,false\n","33960,99999,1929,11,2,43.700000762939453,4,41.799999237060547,4,1024.0999755859375,4,,,3.4000000953674316,4,10,4,13,,39,false,,,,,false,false,false,false,false,false\n","30050,99999,1929,10,18,48,4,46.200000762939453,4,997.29998779296875,4,,,6.1999998092651367,4,6,4,8.8999996185302734,,46.900001525878906,false,,,,,false,false,false,false,false,false\n","38110,99999,1929,12,24,47.299999237060547,4,41.700000762939453,4,993.5999755859375,4,,,5.3000001907348633,4,24.700000762939453,4,43.900001525878906,,44.099998474121094,false,,,,,false,false,false,false,false,false\n","31590,99999,1929,11,2,49.700000762939453,4,47.5,4,1017.9000244140625,4,,,5.3000001907348633,4,5.1999998092651367,4,8.8999996185302734,,48,false,,,,,false,false,false,false,false,false\n","38560,99999,1929,11,1,46.5,4,40.200000762939453,4,1028.300048828125,4,,,7.8000001907348633,4,8.8999996185302734,4,8.8999996185302734,,42.099998474121094,false,,,0,,false,false,false,false,false,false\n","37770,99999,1929,10,9,47.299999237060547,4,40.5,4,1014.7000122070312,4,,,4.3000001907348633,4,10,4,13,,41,false,,,0,,false,false,false,false,false,false\n","31590,99999,1929,12,30,40.200000762939453,4,36.700000762939453,4,994.20001220703125,4,,,8.1000003814697266,4,15,4,23.899999618530273,,37,false,,,,,false,false,false,false,false,false\n","38560,99999,1929,10,26,47.599998474121094,4,37.299999237060547,4,1002.7999877929688,4,,,12.399999618530273,4,13.300000190734863,4,18.100000381469727,,42.099998474121094,false,,,0,,false,false,false,false,false,false\n"]}],"source":["comps = BQ_TABLE.split(\".\")\n","BQ_PROJECT_DATASET_TABLE = comps[0] + \":\" + comps[1] + \".\" + comps[2]\n","\n","! bq --location=us extract --destination_format CSV $BQ_PROJECT_DATASET_TABLE $BUCKET_URI/dataset/csv/mydata*.csv"]},{"cell_type":"markdown","source":["#### Check exported CSV files list"],"metadata":{"id":"hXFmoyvZrlRr"}},{"cell_type":"code","source":["IMPORT_FILES = ! gsutil ls $BUCKET_URI/dataset/csv/mydata*.csv\n","print(IMPORT_FILES)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"26kT1LE7oUxF","executionInfo":{"status":"ok","timestamp":1733620528381,"user_tz":-540,"elapsed":3938,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"ec39f169-37fd-4430-d6c4-d597eda03779"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000000.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000001.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000002.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000003.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000004.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000005.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000006.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000007.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000008.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000009.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000010.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000011.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000012.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000013.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000014.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000015.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000016.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000017.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000018.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000019.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000020.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000021.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000022.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000023.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000024.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000025.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000026.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000027.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000028.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000029.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000030.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000031.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000032.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000033.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000034.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000035.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000036.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000037.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000038.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000039.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000040.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000041.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000042.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000043.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000044.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000045.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000046.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000047.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000048.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000049.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000050.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000051.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000052.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000053.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000054.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000055.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000056.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000057.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000058.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000059.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000060.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000061.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000062.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000063.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000064.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000065.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000066.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000067.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000068.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000069.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000070.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000071.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000072.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000073.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000074.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000075.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000076.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000077.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000078.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000079.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000080.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000081.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000082.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000083.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000084.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000085.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000086.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000087.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000088.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000089.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000090.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000091.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000092.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000093.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000094.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000095.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000096.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000097.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000098.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000099.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000100.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000101.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000102.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000103.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000104.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000105.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000106.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000107.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000108.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000109.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000110.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000111.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000112.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000113.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000114.csv']\n"]}]},{"cell_type":"markdown","source":["#### Check the contents of a CSV file"],"metadata":{"id":"Zsuzvu8grpsv"}},{"cell_type":"code","source":["EXAMPLE_FILE = IMPORT_FILES[0]\n","! gsutil cat $EXAMPLE_FILE | head"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rFbad5CRoc6I","executionInfo":{"status":"ok","timestamp":1733617130964,"user_tz":-540,"elapsed":4188,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"de4b7254-ebc1-46da-b354-8f6a8d4acdb4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["station_number,wban_number,year,month,day,mean_temp,num_mean_temp_samples,mean_dew_point,num_mean_dew_point_samples,mean_sealevel_pressure,num_mean_sealevel_pressure_samples,mean_station_pressure,num_mean_station_pressure_samples,mean_visibility,num_mean_visibility_samples,mean_wind_speed,num_mean_wind_speed_samples,max_sustained_wind_speed,max_gust_wind_speed,max_temperature,max_temperature_explicit,min_temperature,min_temperature_explicit,total_precipitation,snow_depth,fog,rain,snow,hail,thunder,tornado\n","39800,99999,1929,12,11,45.5,4,43.5,4,981.4000244140625,4,,,4.3000001907348633,4,19.799999237060547,4,29.899999618530273,,34,false,,,,,false,false,false,false,false,false\n","37770,99999,1929,12,6,47,4,41.299999237060547,4,993.0999755859375,4,,,4.3000001907348633,4,14.300000190734863,4,18.100000381469727,,45,false,,,,,false,false,false,false,false,false\n","31590,99999,1929,12,6,45.799999237060547,4,38.299999237060547,4,974.5,4,,,12.399999618530273,4,24.5,4,36.900001525878906,,43,false,,,0,,false,false,false,false,false,false\n","30910,99999,1929,11,25,49.799999237060547,4,,,986.5,4,,,3.9000000953674316,4,16.399999618530273,4,23.899999618530273,,41,false,,,0.039999999105930328,,false,false,false,false,false,false\n","33790,99999,1929,8,29,62,4,56.299999237060547,4,1015.5999755859375,4,,,9.3000001907348633,4,11.199999809265137,4,18.100000381469727,,54,false,,,0,,false,false,false,false,false,false\n","38110,99999,1929,11,2,47.799999237060547,4,44,4,1027.9000244140625,4,,,6.8000001907348633,4,6,4,13,,42.099998474121094,false,,,,,false,false,false,false,false,false\n","33790,99999,1929,9,17,55.700000762939453,4,52.799999237060547,4,1021.7999877929688,4,,,1.3999999761581421,4,4.3000001907348633,4,5.0999999046325684,,48,false,,,0,,true,true,true,true,true,true\n","30750,99999,1929,12,7,43.299999237060547,4,41.799999237060547,4,966.5999755859375,4,,,6.1999998092651367,4,12.5,4,29.899999618530273,,42.099998474121094,true,,,,,false,false,false,false,false,false\n","34970,99999,1929,8,26,62.599998474121094,4,56,4,1023.5,4,,,5.9000000953674316,4,11,4,13,,59,false,,,0,,false,false,false,false,false,false\n"]}]},{"cell_type":"markdown","metadata":{"id":"create_dataset:tabular,lrg"},"source":["### Create the dataset\n","\n","Learn more about [TabularDataset from CSV files](https://cloud.google.com/vertex-ai/docs/datasets/create-dataset-api#aiplatform_create_dataset_tabular_gcs_sample-python)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SZjrYHO4lqKz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733617192794,"user_tz":-540,"elapsed":2508,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"8ee40bad-65ef-4164-ea25-ade25850a574"},"outputs":[{"output_type":"stream","name":"stdout","text":["gcs_source : ['gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000000.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000001.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000002.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000003.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000004.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000005.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000006.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000007.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000008.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000009.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000010.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000011.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000012.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000013.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000014.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000015.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000016.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000017.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000018.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000019.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000020.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000021.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000022.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000023.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000024.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000025.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000026.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000027.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000028.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000029.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000030.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000031.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000032.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000033.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000034.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000035.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000036.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000037.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000038.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000039.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000040.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000041.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000042.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000043.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000044.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000045.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000046.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000047.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000048.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000049.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000050.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000051.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000052.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000053.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000054.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000055.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000056.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000057.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000058.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000059.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000060.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000061.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000062.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000063.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000064.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000065.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000066.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000067.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000068.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000069.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000070.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000071.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000072.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000073.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000074.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000075.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000076.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000077.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000078.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000079.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000080.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000081.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000082.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000083.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000084.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000085.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000086.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000087.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000088.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000089.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000090.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000091.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000092.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000093.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000094.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000095.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000096.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000097.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000098.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000099.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000100.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000101.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000102.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000103.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000104.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000105.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000106.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000107.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000108.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000109.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000110.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000111.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000112.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000113.csv', 'gs://mlops-ai-hangsik-1207/dataset/csv/mydata000000000114.csv']\n"]},{"output_type":"stream","name":"stderr","text":["INFO:google.cloud.aiplatform.datasets.dataset:Creating TabularDataset\n","INFO:google.cloud.aiplatform.datasets.dataset:Create TabularDataset backing LRO: projects/721521243942/locations/us-central1/datasets/377506872036425728/operations/1988767392183877632\n","INFO:google.cloud.aiplatform.datasets.dataset:TabularDataset created. Resource name: projects/721521243942/locations/us-central1/datasets/377506872036425728\n","INFO:google.cloud.aiplatform.datasets.dataset:To use this TabularDataset in another session:\n","INFO:google.cloud.aiplatform.datasets.dataset:ds = aiplatform.TabularDataset('projects/721521243942/locations/us-central1/datasets/377506872036425728')\n"]},{"output_type":"stream","name":"stdout","text":["projects/721521243942/locations/us-central1/datasets/377506872036425728\n"]}],"source":["gcs_source = IMPORT_FILES\n","\n","print(f\"gcs_source : {gcs_source}\")\n","\n","# https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.TabularDataset\n","# https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.TabularDataset#google_cloud_aiplatform_TabularDataset_create\n","\n","dataset = aiplatform.TabularDataset.create(\n","    display_name=\"NOAA historical weather data - csv dataset\",\n","    gcs_source=gcs_source,\n","    labels={\"user_metadata\": BUCKET_URI[5:]},\n",")\n","\n","label_column = \"mean_temp\"\n","\n","print(dataset.resource_name)"]},{"cell_type":"markdown","metadata":{"id":"bq_to_dataset:gsod"},"source":["## Read the BigQuery dataset into a tf.data.Dataset\n","\n","*  [BigQuery TensorFlow reader](https://www.tensorflow.org/io/tutorials/bigquery).\n","*  [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o32ZMlVllqK4","colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"status":"error","timestamp":1733620533933,"user_tz":-540,"elapsed":297,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"f79566c3-4ed3-4ecc-8a34-515d19977d9b"},"outputs":[{"output_type":"error","ename":"NotImplementedError","evalue":"unable to open file: libtensorflow_io.so, from paths: ['/usr/local/lib/python3.10/dist-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/usr/local/lib/python3.10/dist-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZN3tsl8str_util9LowercaseB5cxx11ESt17basic_string_viewIcSt11char_traitsIcEE']","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-c77c9b087c71>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mPROJECT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATASET\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTABLE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIMPORT_FILE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mtf_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_bigquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPROJECT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATASET\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTABLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-c77c9b087c71>\u001b[0m in \u001b[0;36mread_bigquery\u001b[0;34m(project, dataset, table)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# https://www.tensorflow.org/io/api_docs/python/tfio/bigquery/BigQueryClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtensorflow_io_bigquery_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBigQueryClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     read_session = tensorflow_io_bigquery_client.read_session(\n\u001b[1;32m     12\u001b[0m         \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"projects/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mPROJECT_ID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_io/python/ops/bigquery_dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;34m\"\"\"Creates a BigQueryClient to start BigQuery read sessions.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcore_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio_big_query_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     def read_session(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_io/python/ops/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attrb)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_io/python/ops/__init__.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_library\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_library\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_io/python/ops/__init__.py\u001b[0m in \u001b[0;36m_load_library\u001b[0;34m(filename, lib)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0merrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     raise NotImplementedError(\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;34m\"unable to open file: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;34m+\u001b[0m \u001b[0;34mf\"{filename}, from paths: {filenames}\\ncaused by: {errs}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotImplementedError\u001b[0m: unable to open file: libtensorflow_io.so, from paths: ['/usr/local/lib/python3.10/dist-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/usr/local/lib/python3.10/dist-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZN3tsl8str_util9LowercaseB5cxx11ESt17basic_string_viewIcSt11char_traitsIcEE']"]}],"source":["from tensorflow.python.framework import dtypes\n","from tensorflow_io.bigquery import BigQueryClient\n","\n","feature_names = \"station_number,year,month,day\".split(\",\")\n","target_name = \"mean_temp\"\n","\n","def read_bigquery(project, dataset, table):\n","\n","    # https://www.tensorflow.org/io/api_docs/python/tfio/bigquery/BigQueryClient\n","    tensorflow_io_bigquery_client = BigQueryClient()\n","    read_session = tensorflow_io_bigquery_client.read_session(\n","        parent=\"projects/\" + PROJECT_ID,\n","        project_id=project,\n","        dataset_id=dataset,\n","        table_id=table,\n","        selected_fields=feature_names + [target_name],\n","        output_types=[dtypes.string] + [dtypes.int32] * 3 + [dtypes.float32],\n","        requested_streams=2,\n","    )\n","\n","    dataset = read_session.parallel_read_rows()\n","    return dataset\n","\n","\n","PROJECT, DATASET, TABLE = IMPORT_FILE.split(\"/\")[-1].split(\".\")\n","tf_dataset = read_bigquery(PROJECT, DATASET, TABLE)\n","\n","print(tf_dataset.take(1))"]},{"cell_type":"markdown","metadata":{"id":"csv_to_dataset:gsod"},"source":["### Read CSV files into a tf.data.Dataset\n","\n","Alternatively, when your data is in CSV files, you can load the dataset into a tf.data.Dataset using `tf.data.experimental.CsvDataset`, with the following parameters:\n","\n","- `filenames`: A list of one or more CSV files.\n","- `header`: Whether CSV file(s) contain a header.\n","- `select_cols`: Subset of fields (columns) to return.\n","- `record_defaults`: The output types of the corresponding fields.\n","\n","Learn more about [tf.data CsvDataset](https://www.tensorflow.org/api_docs/python/tf/data/experimental/CsvDataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Mfcz6_5lqK4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733620599945,"user_tz":-540,"elapsed":301,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"3f076dbd-58b4-473b-def8-757e9bd0bc0a"},"outputs":[{"output_type":"stream","name":"stdout","text":["<_TakeDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None))>\n"]}],"source":["import tensorflow as tf\n","\n","feature_names = [\"station_number,year,month,day\".split(\",\")]\n","\n","target_name = \"mean_temp\"\n","\n","tf_dataset = tf.data.experimental.CsvDataset(\n","    filenames=IMPORT_FILES,\n","    header=True,\n","    select_cols=feature_names.append(target_name),\n","    record_defaults=[dtypes.string] + [dtypes.int32] * 3 + [dtypes.float32],\n",")\n","\n","print(tf_dataset.take(1))"]},{"cell_type":"markdown","metadata":{"id":"dataframe_to_bq"},"source":["### Create a BigQuery dataset from a pandas dataframe\n","\n","You can create a BigQuery dataset from a pandas dataframe using the BigQuery `create_dataset()` and `load_table_from_dataframe()` methods, as follows:\n","\n","- `create_dataset()`: Creates an empty BigQuery dataset, with the following parameters:\n"," - `dataset_ref`: The `DatasetReference` created from the dataset_id -- e.g., samples.\n","- `load_table_from_dataframe()`: Loads one or more CSV files into a table within the corresponding dataset, with the following parameters:\n"," - `dataframe`: The dataframe.\n"," - `table`: The `TableReference` for the table.\n"," - `job_config`: Specifications on how to load the dataframe data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IXiTChw3lqK5"},"outputs":[],"source":["LOCATION = \"us\"\n","\n","SCHEMA = [\n","    bigquery.SchemaField(\"station_number\", \"STRING\"),\n","    bigquery.SchemaField(\"year\", \"INTEGER\"),\n","    bigquery.SchemaField(\"month\", \"INTEGER\"),\n","    bigquery.SchemaField(\"day\", \"INTEGER\"),\n","    bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n","]\n","\n","\n","DATASET_ID = \"samples\"\n","TABLE_ID = \"gsod\"\n","\n","\n","def create_bigquery_dataset(dataset_id):\n","    dataset = bigquery.Dataset(\n","        bigquery.dataset.DatasetReference(PROJECT_ID, dataset_id)\n","    )\n","    dataset.location = \"us\"\n","\n","    try:\n","        dataset = bqclient.create_dataset(dataset)  # API request\n","        return True\n","    except Exception as err:\n","        print(err)\n","        if err.code != 409:  # http_client.CONFLICT\n","            raise\n","    return False\n","\n","\n","def load_data_into_bigquery(dataframe, dataset_id, table_id):\n","    create_bigquery_dataset(dataset_id)\n","    dataset = bqclient.dataset(dataset_id)\n","    table = dataset.table(table_id)\n","\n","    job_config = bigquery.LoadJobConfig(\n","        # Specify a (partial) schema. All columns are always written to the\n","        # table. The schema is used to assist in data type definitions.\n","        schema=[\n","            bigquery.SchemaField(\"station_number\", \"STRING\"),\n","            bigquery.SchemaField(\"year\", \"INTEGER\"),\n","            bigquery.SchemaField(\"month\", \"INTEGER\"),\n","            bigquery.SchemaField(\"day\", \"INTEGER\"),\n","            bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n","        ],\n","        # Optionally, set the write disposition. BigQuery appends loaded rows\n","        # to an existing table by default, but with WRITE_TRUNCATE write\n","        # disposition it replaces the table with the loaded data.\n","        write_disposition=\"WRITE_TRUNCATE\",\n","    )\n","\n","    NEW_BQ_TABLE = f\"{PROJECT_ID}.{dataset_id}.{table_id}\"\n","\n","    job = bqclient.load_table_from_dataframe(\n","        dataframe, NEW_BQ_TABLE, job_config=job_config\n","    )  # Make an API request.\n","    job.result()  # Wait for the job to complete.\n","\n","    table = bqclient.get_table(NEW_BQ_TABLE)  # Make an API request.\n","    print(\n","        \"Loaded {} rows and {} columns to {}\".format(\n","            table.num_rows, len(table.schema), NEW_BQ_TABLE\n","        )\n","    )\n","\n","\n","load_data_into_bigquery(dataframe, DATASET_ID, TABLE_ID)"]},{"cell_type":"markdown","metadata":{"id":"csv_to_bq"},"source":["### Create a BigQuery dataset from CSV files\n","\n","You can create a BigQuery dataset from CSV files using the BigQuery `create_dataset()` and `load_table_from_uri()` methods, as follows:\n","\n","- `create_dataset()`: Creates an empty BigQuery dataset, with the following parameters:\n"," - `dataset_ref`: The `DatasetReference` created from the dataset_id -- e.g., samples.\n","- `load_table_from_uri()`: Loads one or more CSV files into a table within the corresponding dataset, with the following parameters:\n"," - `url`: A set of one or more CVS files in Cloud Storage storage.\n"," - `table`: The `TableReference` for the table.\n"," - `job_config`: Specifications on how to load the CSV data.\n","\n","Learn more about [Importing CSV data into BigQuery](https://www.tensorflow.org/io/tutorials/bigquery#import_census_data_into_bigquery)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7xEml9LBlqK5"},"outputs":[],"source":["LOCATION = \"us\"\n","\n","CSV_SCHEMA = [\n","    bigquery.SchemaField(\"station_number\", \"STRING\"),\n","    bigquery.SchemaField(\"wban_number\", \"STRING\"),\n","    bigquery.SchemaField(\"year\", \"INTEGER\"),\n","    bigquery.SchemaField(\"month\", \"INTEGER\"),\n","    bigquery.SchemaField(\"day\", \"INTEGER\"),\n","    bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n","    bigquery.SchemaField(\"num_mean_temp_samples\", \"INTEGER\"),\n","    bigquery.SchemaField(\"mean_dew_point\", \"FLOAT\"),\n","    bigquery.SchemaField(\"num_mean_dew_point_samples\", \"INTEGER\"),\n","    bigquery.SchemaField(\"mean_sealevel_pressure\", \"FLOAT\"),\n","    bigquery.SchemaField(\"num_mean_sealevel_pressure_samples\", \"INTEGER\"),\n","    bigquery.SchemaField(\"mean_station_pressure\", \"FLOAT\"),\n","    bigquery.SchemaField(\"num_mean_station_pressure_samples\", \"INTEGER\"),\n","    bigquery.SchemaField(\"mean_visibility\", \"FLOAT\"),\n","    bigquery.SchemaField(\"num_mean_visibility_samples\", \"INTEGER\"),\n","    bigquery.SchemaField(\"mean_wind_speed\", \"FLOAT\"),\n","    bigquery.SchemaField(\"num_mean_wind_speed_samples\", \"INTEGER\"),\n","    bigquery.SchemaField(\"max_sustained_wind_speed\", \"FLOAT\"),\n","    bigquery.SchemaField(\"max_gust_wind_speed\", \"FLOAT\"),\n","    bigquery.SchemaField(\"max_temperature\", \"FLOAT\"),\n","    bigquery.SchemaField(\"max_temperature_explicit\", \"BOOLEAN\"),\n","    bigquery.SchemaField(\"min_temperature\", \"FLOAT\"),\n","    bigquery.SchemaField(\"min_temperature_explicit\", \"BOOLEAN\"),\n","    bigquery.SchemaField(\"total_percipitation\", \"FLOAT\"),\n","    bigquery.SchemaField(\"snow_depth\", \"FLOAT\"),\n","    bigquery.SchemaField(\"fog\", \"BOOLEAN\"),\n","    bigquery.SchemaField(\"rain\", \"BOOLEAN\"),\n","    bigquery.SchemaField(\"snow\", \"BOOLEAN\"),\n","    bigquery.SchemaField(\"hail\", \"BOOLEAN\"),\n","    bigquery.SchemaField(\"thunder\", \"BOOLEAN\"),\n","    bigquery.SchemaField(\"tornado\", \"BOOLEAN\"),\n","]\n","\n","\n","DATASET_ID = \"samples\"\n","TABLE_ID = \"gsod\"\n","\n","\n","def load_data_into_bigquery(url, dataset_id, table_id):\n","    create_bigquery_dataset(dataset_id)\n","    dataset = bqclient.dataset(dataset_id)\n","    table = dataset.table(table_id)\n","\n","    job_config = bigquery.LoadJobConfig()\n","    job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n","    job_config.source_format = bigquery.SourceFormat.CSV\n","    job_config.schema = CSV_SCHEMA\n","    job_config.skip_leading_rows = 1  # heading\n","\n","    load_job = bqclient.load_table_from_uri(url, table, job_config=job_config)\n","    print(\"Starting job {}\".format(load_job.job_id))\n","\n","    load_job.result()  # Waits for table load to complete.\n","    print(\"Job finished.\")\n","\n","    destination_table = bqclient.get_table(table)\n","    print(\"Loaded {} rows.\".format(destination_table.num_rows))\n","\n","\n","load_data_into_bigquery(IMPORT_FILES, DATASET_ID, TABLE_ID)"]},{"cell_type":"markdown","metadata":{"id":"bq_to_xgboost"},"source":["### Read BigQuery table into XGboost DMatrix\n","\n","Currently, there is no direct data feeding connector between BigQuery and the open source XGBoost. The BigQuery ML service has a built-in XGBoost training module.\n","\n","Alernatively, you extract the data either as a pandas dataframe or as CSV files. The extracted data is then given as an input to a `DMatrix` object when training the model.\n","\n","Learn more about [Getting started with built-in XGBoost](https://cloud.google.com/ai-platform/training/docs/algorithms/xgboost-start)."]},{"cell_type":"markdown","metadata":{"id":"pandas_to_xgboost:gsod"},"source":["### Read pandas table into XGboost DMatrix\n","\n","Next, you load the pandas dataframe into a `DMatrix` object. XGBoost does not support non-numeric inputs. Any column that is categorical need to be one-hot encoded prior to loading the dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v6oRkbMilqK6"},"outputs":[],"source":["dataframe[\"station_number\"] = pd.to_numeric(dataframe[\"station_number\"])\n","labels = dataframe[\"mean_temp\"]\n","data = dataframe.drop([\"mean_temp\"], axis=1)\n","\n","dtrain = xgb.DMatrix(data, label=labels)"]},{"cell_type":"markdown","metadata":{"id":"csv_to_xgboost:gsod"},"source":["### Read CSV files into XGboost DMatrix\n","\n","Currently, there is no Cloud Storage support in XGBoost. If you use CSV files for input, you need to download them locally."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Ulk0xh0lqK6"},"outputs":[],"source":["! gsutil cp $EXAMPLE_FILE data.csv\n","\n","dtrain = xgb.DMatrix(\"data.csv?format=csv&label_column=4\")"]},{"cell_type":"markdown","source":["### Useful BigQuery Operations"],"metadata":{"id":"p4vqv23HqkGb"}},{"cell_type":"markdown","metadata":{"id":"bq_view"},"source":["#### Create a view of the BigQuery dataset\n","\n","Alternatively, you can create a logical view of a BigQuery dataset that has a subset of the fields.\n","\n","Learn more about [Creating BigQuery views](https://cloud.google.com/bigquery/docs/views)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7dc142433e50"},"outputs":[],"source":["# Set dataset name and view name in BigQuery\n","BQ_MY_DATASET = \"[your-dataset-name]\"\n","BQ_MY_TABLE = \"[your-view-name]\"\n","\n","# Otherwise, use the default names\n","if (\n","    BQ_MY_DATASET == \"\"\n","    or BQ_MY_DATASET is None\n","    or BQ_MY_DATASET == \"[your-dataset-name]\"\n","):\n","    BQ_MY_DATASET = \"mlops_dataset\"\n","\n","if BQ_MY_TABLE == \"\" or BQ_MY_TABLE is None or BQ_MY_TABLE == \"[your-view-name]\":\n","    BQ_MY_TABLE = \"mlops_view\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"erdPT_0tlqK0"},"outputs":[],"source":["# Create the resources\n","! bq --location=US mk -d \\\n","$PROJECT_ID:$BQ_MY_DATASET\n","\n","sql_script = f'''\n","CREATE OR REPLACE VIEW `{PROJECT_ID}.{BQ_MY_DATASET}.{BQ_MY_TABLE}`\n","AS SELECT station_number,year,month,day,mean_temp FROM `{BQ_TABLE}`\n","'''\n","print(sql_script)\n","\n","query = bqclient.query(sql_script)"]},{"cell_type":"markdown","metadata":{"id":"bq_to_dataframe:gsod"},"source":["#### Read the BigQuery dataset into a pandas dataframe\n","\n","Next, you read a sample of the dataset into a pandas dataframe using BigQuery `list_rows()` and `to_dataframe()` method, as follows:\n","\n","- `list_rows()`: Performs a query on the specified table and returns a row iterator to the query results. Optionally specify:\n"," - `selected_fields`: Subset of fields (columns) to return.\n"," - `max_results`: The maximum number of rows to return. Same as SQL LIMIT command.\n","\n","\n","- `rows.to_dataframe()`: Invokes the row iterator and reads in the data into a pandas dataframe.\n","\n","Learn more about [Loading BigQuery table into a dataframe](https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n2HUgKFAlqK0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733617457975,"user_tz":-540,"elapsed":985,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"3756459a-6fb8-4de9-eca7-d590c6dc97cb"},"outputs":[{"output_type":"stream","name":"stdout","text":["  station_number  year  month  day  mean_temp\n","0          39800  1929     12   11  45.500000\n","1          37770  1929     12    6  47.000000\n","2          31590  1929     12    6  45.799999\n","3          30910  1929     11   25  49.799999\n","4          33790  1929      8   29  62.000000\n"]}],"source":["# Download the table.\n","table = bigquery.TableReference.from_string(BQ_TABLE)\n","\n","rows = bqclient.list_rows(\n","    table,\n","    max_results=500,\n","    selected_fields=[\n","        bigquery.SchemaField(\"station_number\", \"STRING\"),\n","        bigquery.SchemaField(\"year\", \"INTEGER\"),\n","        bigquery.SchemaField(\"month\", \"INTEGER\"),\n","        bigquery.SchemaField(\"day\", \"INTEGER\"),\n","        bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n","    ],\n",")\n","\n","dataframe = rows.to_dataframe()\n","print(dataframe.head())"]},{"cell_type":"markdown","metadata":{"id":"cleanup:mbsdk"},"source":["### Clean up\n","\n","To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n","project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n","\n","Otherwise, you can delete the individual resources you created in this tutorial:\n","\n","- Vertex AI dataset resource\n","- Cloud Storage Bucket\n","- BigQuery dataset\n","\n","Set `delete_storage` to _True_ to delete the storage resources used in this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"47ad926d84e8"},"outputs":[],"source":["import os\n","\n","# Delete the dataset using the Vertex dataset object\n","dataset.delete()\n","\n","# Delete the temporary BigQuery dataset\n","! bq rm -r -f $PROJECT_ID:$DATASET_ID\n","\n","delete_storage = False\n","if delete_storage or os.getenv(\"IS_TESTING\"):\n","    # Delete the created GCS bucket\n","    ! gsutil rm -r $BUCKET_URI\n","    # Delete the created BigQuery datasets\n","    ! bq rm -r -f $PROJECT_ID:$BQ_MY_DATASET"]}],"metadata":{"colab":{"toc_visible":true,"provenance":[{"file_id":"1Yl48IT7kU2-mUB0p7YNS4E2df74WoVha","timestamp":1733620660659},{"file_id":"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/datasets/get_started_bq_datasets.ipynb","timestamp":1722090438798}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ur8xi4C7S06n"},"outputs":[],"source":["# Copyright 2024 Forusone(shins777@gmail.com)\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."]},{"cell_type":"markdown","metadata":{"id":"JAPoU8Sm5E6e"},"source":["# Compare pipeline runs with Vertex AI Experiments\n","* This code simplifies [Compare pipeline runs with Vertex AI Experiments](https://colab.sandbox.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/experiments/comparing_pipeline_runs.ipynb#scrollTo=JAPoU8Sm5E6e)\n","\n","* [Iris dataset](https://www.tensorflow.org/datasets/catalog/iris) from [TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/overview)."]},{"cell_type":"markdown","metadata":{"id":"xlpUSF8AIj63"},"source":["## Set Configuration"]},{"cell_type":"markdown","metadata":{"id":"i7EUnXsZhAGF"},"source":["### Install Vertex AI SDK for Python and other required packages"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"2b4ef9b72d43","executionInfo":{"status":"ok","timestamp":1735693901735,"user_tz":-540,"elapsed":4376,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["%pip install --upgrade --quiet --user --no-warn-conflicts google-cloud-aiplatform \\\n","                                                          \"kfp==2.0.0\" \\\n","                                                          setuptools"]},{"cell_type":"markdown","metadata":{"id":"LnTLX7UGKql8"},"source":["### Authenticate to access to GCP\n","* Only for Colab in Google Drive\n","* No need to do this process if in Colab Enteprise on Vertex AI.\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"GhWCxscxKyeG","executionInfo":{"status":"ok","timestamp":1735693905656,"user_tz":-540,"elapsed":858,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["# To use markdown for output data from LLM\n","from IPython.display import display, Markdown\n","\n","# Use OAuth to access the GCP environment.\n","import sys\n","if \"google.colab\" in sys.modules:\n","    from google.colab import auth\n","    auth.authenticate_user()\n",""]},{"cell_type":"code","execution_count":3,"metadata":{"id":"set_project_id","cellView":"form","executionInfo":{"status":"ok","timestamp":1735693907753,"user_tz":-540,"elapsed":4,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["# @title Configuration\n","\n","PROJECT_ID = \"ai-hangsik\"  # @param {type:\"string\"}\n","LOCATION = \"us-central1\"  # @param {type:\"string\"}\n","BUCKET_URI = f\"gs://mlops_{PROJECT_ID}_7424\"  # @param {type:\"string\"}"]},{"cell_type":"markdown","source":["## Set Bucket"],"metadata":{"id":"ITeSJkIqPGDk"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"SVwNOMp39Qtz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735693916022,"user_tz":-540,"elapsed":5775,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"3d1b5f75-74c1-42c3-c82e-02bc75efb493"},"outputs":[{"output_type":"stream","name":"stdout","text":["Creating gs://mlops_ai-hangsik_7424/...\n","ServiceException: 409 A Cloud Storage bucket named 'mlops_ai-hangsik_7424' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"]}],"source":["# @title Create bucket\n","! gsutil mb -l $LOCATION -p {PROJECT_ID} $BUCKET_URI"]},{"cell_type":"code","source":["# @title Service account\n","shell_output = ! gcloud projects describe  $PROJECT_ID\n","project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n","\n","SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n","\n","print(f\"SERVICE_ACCOUNT: {SERVICE_ACCOUNT}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WENMR3pzOhTn","executionInfo":{"status":"ok","timestamp":1735693918890,"user_tz":-540,"elapsed":1455,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"828add09-17f3-4353-ec2c-716c3a14c093"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["SERVICE_ACCOUNT: 721521243942-compute@developer.gserviceaccount.com\n"]}]},{"cell_type":"code","source":["# @title Set access to the bucket\n","! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n","! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x0klQ108OjNm","executionInfo":{"status":"ok","timestamp":1735693925865,"user_tz":-540,"elapsed":5441,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"3bc19b20-9580-4f23-8270-ff3a402c37c2"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["No changes made to gs://mlops_ai-hangsik_7424/\n","No changes made to gs://mlops_ai-hangsik_7424/\n"]}]},{"cell_type":"markdown","metadata":{"id":"fXUqOdIaLbjf"},"source":["##Data Preparation\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"9fYX14c0LfmU","colab":{"base_uri":"https://localhost:8080/"},"outputId":"782b0e16-e910-4251-e1a9-c18944f7642c","executionInfo":{"status":"ok","timestamp":1735693931291,"user_tz":-540,"elapsed":3829,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Copying gs://cloud-samples-data/ai-platform/iris/classification/evaluate.csv [Content-Type=text/csv]...\n","Copying gs://cloud-samples-data/ai-platform/iris/classification/train.csv [Content-Type=text/csv]...\n","Copying gs://cloud-samples-data/ai-platform/iris/iris_data.csv [Content-Type=application/octet-stream]...\n","Copying gs://cloud-samples-data/ai-platform/iris/iris_predict.csv [Content-Type=text/csv]...\n","- [4 files][  5.4 KiB/  5.4 KiB]                                                \n","==> NOTE: You are performing a sequence of gsutil operations that may\n","run significantly faster if you instead use gsutil -m cp ... Please\n","see the -m section under \"gsutil help options\" for further information\n","about when gsutil -m can be advantageous.\n","\n","Copying gs://cloud-samples-data/ai-platform/iris/iris_target.csv [Content-Type=application/octet-stream]...\n","Copying gs://cloud-samples-data/ai-platform/iris/iris_test.csv [Content-Type=text/csv]...\n","Copying gs://cloud-samples-data/ai-platform/iris/iris_training.csv [Content-Type=text/csv]...\n","\\ [7 files][  8.4 KiB/  8.4 KiB]                                                \n","Operation completed over 7 objects/8.4 KiB.                                      \n"]}],"source":["# @title Download training dataset\n","DATASET_URI = \"gs://cloud-samples-data/ai-platform/iris\"\n","!gsutil cp -r $DATASET_URI $BUCKET_URI"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"pRUOFELefqf1","executionInfo":{"status":"ok","timestamp":1735693936082,"user_tz":-540,"elapsed":2668,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["# @title Import libraries and define constants\n","\n","# General\n","import time\n","import uuid\n","\n","#-----------------------------\n","import kfp.compiler as compiler\n","# Pipeline Experiments\n","import kfp.dsl as dsl\n","# Vertex AI\n","from google.cloud import aiplatform as vertex_ai\n","from google.cloud.aiplatform_v1.types.pipeline_state import PipelineState\n","from kfp.dsl import Metrics, Model, Output, component\n","\n","#-----------------------------\n","import logging\n","logger = logging.getLogger(\"logger\")\n","logging.basicConfig(level=logging.INFO)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"OAY0QKZD8qNP","executionInfo":{"status":"ok","timestamp":1735693936089,"user_tz":-540,"elapsed":5,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["# @title Set Experiments and Pipeline\n","\n","# Experiments\n","TASK = \"classification\"\n","MODEL_TYPE = \"xgboost\"\n","EXPERIMENT_NAME = f\"{PROJECT_ID}-{TASK}-{MODEL_TYPE}-{uuid.uuid1()}\"\n","\n","# Pipeline\n","PIPELINE_TEMPLATE_FILE = \"pipeline.json\"\n","PIPELINE_URI = f\"{BUCKET_URI}/pipelines\"\n","TRAIN_URI = f\"{BUCKET_URI}/iris/iris_data.csv\"\n","LABEL_URI = f\"{BUCKET_URI}/iris/iris_target.csv\"\n","MODEL_URI = f\"{BUCKET_URI}/model\""]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"Rc9qgjA6P9fN"}},{"cell_type":"code","execution_count":10,"metadata":{"id":"Nz0nasrh8T3c","executionInfo":{"status":"ok","timestamp":1735693938578,"user_tz":-540,"elapsed":4,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["# @title Initialize Vertex AI SDK for Python\n","vertex_ai.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"]},{"cell_type":"markdown","metadata":{"id":"container:training,prediction,xgboost"},"source":["### Set pre-built containers\n","\n","* [Pre-built containers for training](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers).\n","* [Pre-built containers for prediction](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers)."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"XujRA5ueox9U","executionInfo":{"status":"ok","timestamp":1735693940534,"user_tz":-540,"elapsed":5,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["TRAIN_IMAGE = vertex_ai.helpers.get_prebuilt_prediction_container_uri(\n","    framework=\"xgboost\", framework_version=\"1.1\", accelerator=\"cpu\"\n",")"]},{"cell_type":"markdown","metadata":{"id":"t1NLYz1R-KWv"},"source":["## Formalize the training as pipeline component\n"]},{"cell_type":"markdown","metadata":{"id":"jnfKxpj0-Z0H"},"source":["Before you start running your pipeline experiments, you have to formalize your training as pipeline component.\n","\n","To do that, build the pipeline by using the `kfp.dsl.component` decorator to convert your training task into a pipeline component. The below example specifies a base image for the component (python:3.8)."]},{"cell_type":"code","execution_count":12,"metadata":{"id":"jv_-vU46_eFN","executionInfo":{"status":"ok","timestamp":1735693942889,"user_tz":-540,"elapsed":27,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["@component(\n","    base_image=\"python:3.8\",\n","    packages_to_install=[\n","        \"numpy==1.18.5\",\n","        \"pandas==1.0.4\",\n","        \"scikit-learn==0.23.1\",\n","        \"xgboost==1.1.1\",\n","    ],\n",")\n","def custom_trainer(\n","    train_uri: str,\n","    label_uri: str,\n","    max_depth: int,\n","    learning_rate: float,\n","    boost_rounds: int,\n","    model_uri: str,\n","    metrics: Output[Metrics],\n","    model_metadata: Output[Model],\n","):\n","\n","    # import libraries\n","    import logging\n","    import uuid\n","    from pathlib import Path as path\n","\n","    import pandas as pd\n","    import xgboost as xgb\n","    from sklearn.metrics import accuracy_score\n","    from sklearn.model_selection import train_test_split\n","\n","    # variables\n","    gs_prefix = \"gs://\"\n","    gcsfuse_prefix = \"/gcs/\"\n","    train_path = train_uri.replace(gs_prefix, gcsfuse_prefix)\n","    label_path = label_uri.replace(gs_prefix, gcsfuse_prefix)\n","    model_path = model_uri.replace(gs_prefix, gcsfuse_prefix)\n","\n","    def get_logger():\n","        \"\"\"\n","        Get the logger\n","        \"\"\"\n","        logger = logging.getLogger(__name__)\n","        logger.setLevel(logging.INFO)\n","        handler = logging.StreamHandler()\n","        handler.setFormatter(\n","            logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n","        )\n","        logger.addHandler(handler)\n","        return logger\n","\n","    def get_data(\n","        train_path: str, label_path: str\n","    ) -> (xgb.DMatrix, pd.DataFrame, pd.DataFrame):\n","        \"\"\"\n","        Get the data\n","        Args:\n","            train_path: the path of the train data\n","            label_path: the path of the label data\n","        Returns:\n","            the train data and the label data\n","        \"\"\"\n","        # Load data into pandas, then use `.values` to get NumPy arrays\n","        data = pd.read_csv(train_path).values\n","        labels = pd.read_csv(label_path).values\n","\n","        # Convert one-column 2D array into 1D array for use with XGBoost\n","        labels = labels.reshape((labels.size,))\n","        train_data, test_data, train_labels, test_labels = train_test_split(\n","            data, labels, test_size=0.2, random_state=7\n","        )\n","\n","        # Load data into DMatrix object\n","        dtrain = xgb.DMatrix(train_data, label=train_labels)\n","        return dtrain, test_data, test_labels\n","\n","    def train_model(max_depth: int, eta: int, boost_rounds, dtrain: xgb.DMatrix):\n","        \"\"\"\n","        Train the model\n","        Args:\n","            max_depth: the max depth of the model\n","            eta: the eta of the model\n","            boost_rounds: the boost rounds of the model\n","            dtrain: the train data\n","        Returns:\n","            the trained model\n","        \"\"\"\n","        # Train XGBoost model\n","        param = {\"max_depth\": max_depth, \"eta\": eta}\n","        model = xgb.train(param, dtrain, num_boost_round=boost_rounds)\n","        return model\n","\n","    def evaluate_model(model, test_data, test_labels):\n","        \"\"\"\n","        Evaluate the model\n","        Args:\n","            model: the trained model\n","            test_data: the test data\n","            test_labels: the test labels\n","        Returns:\n","            the accuracy of the model\n","        \"\"\"\n","        dtest = xgb.DMatrix(test_data)\n","        pred = model.predict(dtest)\n","        predictions = [round(value) for value in pred]\n","        # Evaluate predictions\n","        accuracy = accuracy_score(test_labels, predictions)\n","        return accuracy\n","\n","    def save_model(model, model_path):\n","        \"\"\"\n","        Save the model\n","        Args:\n","            model: the trained model\n","            model_path: the path of the model\n","        \"\"\"\n","        model_id = str(uuid.uuid1())\n","        model_path = f\"{model_path}/{model_id}/model.bst\"\n","        path(model_path).parent.mkdir(parents=True, exist_ok=True)\n","        model.save_model(model_path)\n","\n","    # Main ----------------------------------------------\n","\n","    dtrain, test_data, test_labels = get_data(train_path, label_path)\n","    model = train_model(max_depth, learning_rate, boost_rounds, dtrain)\n","    accuracy = evaluate_model(model, test_data, test_labels)\n","    save_model(model, model_path)\n","\n","    # Metadata ------------------------------------------\n","    metrics.log_metric(\"accurancy\", accuracy)\n","    model_metadata.uri = model_uri"]},{"cell_type":"markdown","metadata":{"id":"U1UiTZhkVoFM"},"source":["## Build a pipeline\n","\n","Next, create the pipelineJob in associated project."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"9Gfr6pNLU-dB","executionInfo":{"status":"ok","timestamp":1735693949025,"user_tz":-540,"elapsed":5,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["@dsl.pipeline(name=\"custom-training-pipeline\")\n","def pipeline(\n","    train_uri: str,\n","    label_uri: str,\n","    max_depth: int,\n","    learning_rate: float,\n","    boost_rounds: int,\n","    model_uri: str,\n","):\n","\n","    custom_trainer(\n","        train_uri=train_uri,\n","        label_uri=label_uri,\n","        max_depth=max_depth,\n","        learning_rate=learning_rate,\n","        boost_rounds=boost_rounds,\n","        model_uri=model_uri,\n","    )"]},{"cell_type":"markdown","metadata":{"id":"RkfZ7qVAVjBO"},"source":["### Compile the pipeline\n","\n","Next, compile the pipeline to a JSON file."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"oYlLBGUSVibG","executionInfo":{"status":"ok","timestamp":1735693951399,"user_tz":-540,"elapsed":5,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"pipeline.json\")"]},{"cell_type":"markdown","metadata":{"id":"95vG4-zPWc0B"},"source":["## Submit and track pipeline runs"]},{"cell_type":"markdown","metadata":{"id":"ZNb6kZ2l5t-O"},"source":["### Submit pipeline runs\n","\n","Now that you have the pipeline, define its training configuration depending on the defined parameters. In the following example, you can see how to submit several pipeline runs."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"XPy0Jc8xXgpa","executionInfo":{"status":"ok","timestamp":1735693953215,"user_tz":-540,"elapsed":6,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["runs = [\n","    {\"max_depth\": 4, \"learning_rate\": 0.2, \"boost_rounds\": 10},\n","    # {\"max_depth\": 5, \"learning_rate\": 0.3, \"boost_rounds\": 20},\n","    # {\"max_depth\": 3, \"learning_rate\": 0.1, \"boost_rounds\": 30},\n","    # {\"max_depth\": 6, \"learning_rate\": 0.5, \"boost_rounds\": 40},\n","    # {\"max_depth\": 5, \"learning_rate\": 0.4, \"boost_rounds\": 30},\n","]"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"G0hm1no_WY8o","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c0e5808c-fdfa-420f-91b9-394afa432b77","executionInfo":{"status":"ok","timestamp":1735693958898,"user_tz":-540,"elapsed":3847,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n","INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/721521243942/locations/us-central1/pipelineJobs/custom-training-pipeline-20250101011235\n","INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n","INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/721521243942/locations/us-central1/pipelineJobs/custom-training-pipeline-20250101011235')\n","INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n","https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20250101011235?project=721521243942\n","INFO:google.cloud.aiplatform.metadata.experiment_resources:Associating projects/721521243942/locations/us-central1/pipelineJobs/custom-training-pipeline-20250101011235 to Experiment: ai-hangsik-classification-xgboost-708f479a-c7dd-11ef-ae03-0242ac1c000c\n"]}],"source":["for i, run in enumerate(runs):\n","\n","    job = vertex_ai.PipelineJob(\n","        display_name=f\"{EXPERIMENT_NAME}-pipeline-run-{i}\",\n","        template_path=PIPELINE_TEMPLATE_FILE,\n","        pipeline_root=PIPELINE_URI,\n","        parameter_values={\n","            \"train_uri\": TRAIN_URI,\n","            \"label_uri\": LABEL_URI,\n","            \"model_uri\": MODEL_URI,\n","            **run,\n","        },\n","    )\n","    job.submit(experiment=EXPERIMENT_NAME)"]},{"cell_type":"markdown","metadata":{"id":"O8TV4q535c2M"},"source":["### Check Pipeline run states\n","\n","Vertex AI SDK provides you `get_experiment_df` method to monitor the status of pipeline runs. You can use it either to return parameters and metrics of the pipeline runs in the Vertex AI Experiment or in combination with `get` method of `PipelineJob` to return the pipeline job in Vertex AI Pipeline.\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"dlCEJKfH5xR7","colab":{"base_uri":"https://localhost:8080/","height":153},"outputId":"2ebdff4d-29ba-4461-e5fa-fbf1f6e3b64a","executionInfo":{"status":"ok","timestamp":1735694077016,"user_tz":-540,"elapsed":3469,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                     experiment_name  \\\n","0  ai-hangsik-classification-xgboost-708f479a-c7d...   \n","\n","                                  run_name            run_type     state  \\\n","0  custom-training-pipeline-20250101011235  system.PipelineRun  COMPLETE   \n","\n","   param.boost_rounds                                param.train_uri  \\\n","0                10.0  gs://mlops_ai-hangsik_7424/iris/iris_data.csv   \n","\n","                                   param.label_uri  \\\n","0  gs://mlops_ai-hangsik_7424/iris/iris_target.csv   \n","\n","                    param.model_uri  param.max_depth  param.learning_rate  \\\n","0  gs://mlops_ai-hangsik_7424/model              4.0                  0.2   \n","\n","   metric.accurancy  \n","0               0.9  "],"text/html":["\n","  <div id=\"df-14e4a1de-474a-44d0-9d04-c5ef2a347b63\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>experiment_name</th>\n","      <th>run_name</th>\n","      <th>run_type</th>\n","      <th>state</th>\n","      <th>param.boost_rounds</th>\n","      <th>param.train_uri</th>\n","      <th>param.label_uri</th>\n","      <th>param.model_uri</th>\n","      <th>param.max_depth</th>\n","      <th>param.learning_rate</th>\n","      <th>metric.accurancy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ai-hangsik-classification-xgboost-708f479a-c7d...</td>\n","      <td>custom-training-pipeline-20250101011235</td>\n","      <td>system.PipelineRun</td>\n","      <td>COMPLETE</td>\n","      <td>10.0</td>\n","      <td>gs://mlops_ai-hangsik_7424/iris/iris_data.csv</td>\n","      <td>gs://mlops_ai-hangsik_7424/iris/iris_target.csv</td>\n","      <td>gs://mlops_ai-hangsik_7424/model</td>\n","      <td>4.0</td>\n","      <td>0.2</td>\n","      <td>0.9</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-14e4a1de-474a-44d0-9d04-c5ef2a347b63')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-14e4a1de-474a-44d0-9d04-c5ef2a347b63 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-14e4a1de-474a-44d0-9d04-c5ef2a347b63');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"vertex_ai\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"experiment_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"ai-hangsik-classification-xgboost-708f479a-c7dd-11ef-ae03-0242ac1c000c\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"run_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"custom-training-pipeline-20250101011235\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"run_type\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"system.PipelineRun\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"state\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"COMPLETE\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"param.boost_rounds\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 10.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"param.train_uri\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"gs://mlops_ai-hangsik_7424/iris/iris_data.csv\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"param.label_uri\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"gs://mlops_ai-hangsik_7424/iris/iris_target.csv\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"param.model_uri\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"gs://mlops_ai-hangsik_7424/model\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"param.max_depth\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 4.0,\n        \"max\": 4.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          4.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"param.learning_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.2,\n        \"max\": 0.2,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"metric.accurancy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.9,\n        \"max\": 0.9,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":17}],"source":["# see state of all pipelineJob\n","vertex_ai.get_experiment_df(EXPERIMENT_NAME)"]},{"cell_type":"markdown","metadata":{"id":"98c022ca36b4"},"source":["The pipeline runs in the Vertex AI Experiment is monitored based on pipeline run status."]},{"cell_type":"code","execution_count":18,"metadata":{"id":"FA9W85vs7LLD","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f01f53ea-b938-454a-d0cc-1afeacde99b3","executionInfo":{"status":"ok","timestamp":1735694104178,"user_tz":-540,"elapsed":2335,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Pipeline experiment runs have completed\n"]}],"source":["while True:\n","    pipeline_experiments_df = vertex_ai.get_experiment_df(EXPERIMENT_NAME)\n","    if any(\n","        pipeline_state != \"COMPLETE\" for pipeline_state in pipeline_experiments_df.state\n","    ):\n","        print(\"Pipeline runs are still running...\")\n","        if any(\n","            pipeline_state == \"FAILED\"\n","            for pipeline_state in pipeline_experiments_df.state\n","        ):\n","            print(\"At least one Pipeline run failed\")\n","            break\n","    else:\n","        print(\"Pipeline experiment runs have completed\")\n","        break\n","    time.sleep(60)"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"ISsK9Msi-Kqs","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fe8796fe-94dc-4561-8c40-adbd48417037","executionInfo":{"status":"ok","timestamp":1735694110371,"user_tz":-540,"elapsed":3113,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Pipeline job name:  projects/721521243942/locations/us-central1/pipelineJobs/custom-training-pipeline-20250101011235\n","Pipeline Run UI link:  https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-training-pipeline-20250101011235?project=721521243942\n"]}],"source":["# Get the PipelineJob resource using the experiment run name\n","pipeline_experiments_df = vertex_ai.get_experiment_df(EXPERIMENT_NAME)\n","job = vertex_ai.PipelineJob.get(pipeline_experiments_df.run_name[0])\n","print(\"Pipeline job name: \", job.resource_name)\n","print(\"Pipeline Run UI link: \", job._dashboard_uri())"]},{"cell_type":"markdown","source":["## Load Model"],"metadata":{"id":"v3H20Ro_qNwL"}},{"cell_type":"code","source":["# @title Load the model from GCS\n","import pickle\n","import xgboost as xgb\n","\n","from google.cloud import storage\n","import io\n","# Initialize a Cloud Storage client\n","storage_client = storage.Client(project=PROJECT_ID)\n","\n","BUCKET_NAME = \"mlops_ai-hangsik_7424\"\n","MODEL_FILE = \"model/a77d55bc-c7dd-11ef-9b0b-661500983924/model.bst\"\n","\n","bucket = storage_client.bucket(BUCKET_NAME)\n","blob = bucket.blob(MODEL_FILE)\n","\n","# Download the model file to a local temporary file\n","local_model_path = \"./model.bst\"\n","blob.download_to_filename(local_model_path)\n","\n","# Load the XGBoost model using xgboost.Booster.load_model\n","model = xgb.Booster()  # Create a Booster object\n","model.load_model(local_model_path) # Load the model from the t\n"],"metadata":{"id":"V25ixDwJjarK","executionInfo":{"status":"ok","timestamp":1735697980907,"user_tz":-540,"elapsed":643,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["# @title Predict\n","dtest = xgb.DMatrix([[1.1, 4.5, 1.4, 2.2]])  # Example input features\n","prediction = model.predict(dtest)\n","\n","print(f\"Prediction: {prediction}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gB4W8agZmNaR","executionInfo":{"status":"ok","timestamp":1735698003926,"user_tz":-540,"elapsed":4,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"58020781-0fab-4a69-d899-59f036ec1952"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction: [0.05681667]\n"]}]},{"cell_type":"code","source":["# @title Predict\n","\n","# Prepare new data for prediction\n","\"\"\"\n","5.4,3.9,1.7,0.4,0\n","7.7,3.8,6.7,2.2,2\n","6.3,3.3,4.7,1.6,1\n","6.8,3.2,5.9,2.3,2\n","7.6,3.0,6.6,2.1,2\n","6.4,3.2,5.3,2.3,2\n","\n","\"\"\"\n","\n","new_data = [[6.3,3.3,4.7,1.6]]\n","dnew = xgb.DMatrix(new_data)\n","\n","# Make predictions\n","predictions = model.predict(dnew)\n","\n","# Get predicted class labels (0, 1, or 2 for Iris species)\n","predicted_labels = [int(round(pred)) for pred in predictions]\n","\n","print(f\"Predicted labels: {predicted_labels}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M4b_Rb5AoFag","executionInfo":{"status":"ok","timestamp":1735698065692,"user_tz":-540,"elapsed":5,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"c4e2bf0c-a2a7-476d-c2d1-6a6106160cc3"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted labels: [1]\n"]}]},{"cell_type":"markdown","metadata":{"id":"TpV-iwP9qw9c"},"source":["## Cleaning up\n","\n","To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n","project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n","\n","Otherwise, you can delete the individual resources you created in this tutorial"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6xbYQn5t5Noe"},"outputs":[],"source":["# Delete the pipeline\n","while True:\n","    for i in range(0, len(runs)):\n","        pipeline_job = vertex_ai.PipelineJob.get(pipeline_experiments_df.run_name[i])\n","        if pipeline_job.state != PipelineState.PIPELINE_STATE_SUCCEEDED:\n","            print(\"Pipeline job is still running...\")\n","            time.sleep(60)\n","        else:\n","            print(\"Pipeline job is complete.\")\n","            pipeline_job.delete()\n","    break\n","\n","# Delete experiment\n","exp = vertex_ai.Experiment(EXPERIMENT_NAME)\n","exp.delete()\n","\n","# Delete the Cloud Storage bucket\n","delete_bucket = False  # Set True for deletion\n","if delete_bucket:\n","    ! gsutil rm -rf {BUCKET_URI}\n","\n","# Remove local files\n","!rm {PIPELINE_TEMPLATE_FILE}"]}],"metadata":{"colab":{"toc_visible":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ur8xi4C7S06n"},"outputs":[],"source":["# Copyright 2021 Google LLC\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."]},{"cell_type":"markdown","metadata":{"id":"JAPoU8Sm5E6e"},"source":["# Metadata : Track artifacts and metrics across Vertex AI Pipelines runs\n","\n","Learn more about [Vertex ML Metadata](https://cloud.google.com/vertex-ai/docs/ml-metadata) and [Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction).\n","\n","*   https://colab.sandbox.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/ml_metadata/vertex-pipelines-ml-metadata.ipynb\n","\n","\n","### Dataset\n","\n","This notebook uses scikit-learn to train a model and classify bean types using the [Dry Beans Dataset](https://archive.ics.uci.edu/ml/datasets/Dry+Bean+Dataset) from UCI Machine Learning. This is a tabular dataset that includes measurements and characteristics of seven different types of beans taken from images.\n"]},{"cell_type":"markdown","metadata":{"id":"No17Cw5hgx12"},"source":["## Install Vertex AI SDK"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"wyy5Lbnzg5fi","executionInfo":{"status":"ok","timestamp":1733660993972,"user_tz":-540,"elapsed":4086,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["! pip3 install --upgrade --quiet google-cloud-aiplatform \\\n","                                 kfp"]},{"cell_type":"code","source":["! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N5BD_7PvQDzv","executionInfo":{"status":"ok","timestamp":1733661053798,"user_tz":-540,"elapsed":1946,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"3104a41e-ce12-4143-ac94-6a0d9c00bfa3"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["KFP SDK version: 2.10.1\n"]}]},{"cell_type":"markdown","source":["## Configuration"],"metadata":{"id":"Uz8cDnFWQHNM"}},{"cell_type":"markdown","metadata":{"id":"dmWOrTJ3gx13"},"source":["### Authenticate your notebook environment"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"NyKGtVQjgx13","executionInfo":{"status":"ok","timestamp":1733661121814,"user_tz":-540,"elapsed":17036,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d5c0ef4f-6835-48cd-d6ad-64d2c75f7c67"},"outputs":[{"output_type":"stream","name":"stdout","text":["Updated property [core/project].\n"]}],"source":["import sys\n","from IPython.display import Markdown, display\n","\n","PROJECT_ID=\"ai-hangsik\"\n","LOCATION=\"us-central1\"\n","\n","# For only colab user, no need this process for Colab Enterprise in Vertex AI.\n","if \"google.colab\" in sys.modules:\n","    from google.colab import auth\n","    auth.authenticate_user(project_id=PROJECT_ID)\n","\n","# set project.\n","!gcloud config set project {PROJECT_ID}"]},{"cell_type":"markdown","source":["### Initialize Vertex AI SDK"],"metadata":{"id":"4dvc1w7cQeEk"}},{"cell_type":"code","source":["\n","from google.cloud import aiplatform\n","\n","aiplatform.init(project=PROJECT_ID, location=LOCATION)"],"metadata":{"id":"J-kjglrqQcH1","executionInfo":{"status":"ok","timestamp":1733661275516,"user_tz":-540,"elapsed":297,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["### Create a bucket"],"metadata":{"id":"xXcmhnHsQlTr"}},{"cell_type":"code","source":["# Create a bucket.\n","BUCKET_URI = f\"gs://mlops-{PROJECT_ID}-1209\"\n","! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eVXmwapOQk0V","executionInfo":{"status":"ok","timestamp":1733661211269,"user_tz":-540,"elapsed":3622,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"518599b9-27d5-44f0-feed-710c9e746ca1"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Creating gs://mlops-ai-hangsik-1209/...\n","ServiceException: 409 A Cloud Storage bucket named 'mlops-ai-hangsik-1209' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"]}]},{"cell_type":"code","source":["shell_output = ! gcloud projects describe  $PROJECT_ID\n","project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n","\n","SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n","\n","print(f\"SERVICE_ACCOUNT: {SERVICE_ACCOUNT}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eC9F9UbuQofT","executionInfo":{"status":"ok","timestamp":1733661215533,"user_tz":-540,"elapsed":1525,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"415c580a-93cb-4c79-8cd9-814f8c2faff8"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["SERVICE_ACCOUNT: 721521243942-compute@developer.gserviceaccount.com\n"]}]},{"cell_type":"markdown","source":["### Set access for Service account"],"metadata":{"id":"1OrvDpzjQwEr"}},{"cell_type":"code","source":["! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n","! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yw3dq3ibQxWK","executionInfo":{"status":"ok","timestamp":1733661245998,"user_tz":-540,"elapsed":5037,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"2a12a173-6f80-4dd4-e35e-829eee6daf51"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["No changes made to gs://mlops-ai-hangsik-1209/\n","No changes made to gs://mlops-ai-hangsik-1209/\n"]}]},{"cell_type":"markdown","metadata":{"id":"XoEqT2Y4DJmf"},"source":["### Import libraries and define constants"]},{"cell_type":"markdown","metadata":{"id":"Y9Uo3tifg1kx"},"source":["Import required libraries."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"pRUOFELefqf1","executionInfo":{"status":"ok","timestamp":1733661305017,"user_tz":-540,"elapsed":295,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","from google.cloud import aiplatform, aiplatform_v1beta1\n","from google.cloud.aiplatform import pipeline_jobs\n","from kfp import compiler, dsl\n","from kfp.dsl import (Artifact, Dataset, Input, Metrics, Model, Output,\n","                     OutputPath, component)"]},{"cell_type":"markdown","metadata":{"id":"xtXZWmYqJ1bh"},"source":["Define some constants"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"JIOrI-hoJ46P","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1733661377969,"user_tz":-540,"elapsed":298,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"1115f611-a2f6-419b-a7ba-3c85e5b1186d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'gs://mlops-ai-hangsik-1209/pipeline/custom/metadata'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}],"source":["# PATH = get_ipython().run_line_magic(\"env\", \"PATH\")\n","# %env PATH={PATH}:/home/jupyter/.local/bin\n","# REGION = \"asia-northeast3\"\n","\n","PIPELINE_ROOT = f\"{BUCKET_URI}/pipeline/custom/metadata\"\n","PIPELINE_ROOT"]},{"cell_type":"markdown","metadata":{"id":"l1YW2pgyegFP"},"source":["## Create a pipeline\n"]},{"cell_type":"markdown","metadata":{"id":"KPY41M9_AhZU"},"source":["### Create and define Python function based components"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"RiQuMv4bmpuV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733662273757,"user_tz":-540,"elapsed":293,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"bfd826be-b135-48ec-b48b-0115883ac82d"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-27-459c753a459f>:1: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n","  @component(\n","<ipython-input-27-459c753a459f>:7: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n","  def get_dataframe(\n"]}],"source":["@component(\n","    packages_to_install=[\"google-cloud-bigquery[pandas]\", \"pyarrow\"],\n","    base_image=\"python:3.10\",\n","    output_component_file=\"get_dataframe.yaml\",\n",")\n","\n","def get_dataframe(\n","    project_id: str, bq_table: str, output_data_path: OutputPath(\"Dataset\")\n","):\n","    from google.cloud import bigquery\n","\n","    bqclient = bigquery.Client(project=project_id)\n","    table = bigquery.TableReference.from_string(bq_table)\n","    rows = bqclient.list_rows(table)\n","    dataframe = rows.to_dataframe(\n","        create_bqstorage_client=True,\n","    )\n","    dataframe = dataframe.sample(frac=1, random_state=2)\n","    dataframe.to_csv(output_data_path)"]},{"cell_type":"markdown","metadata":{"id":"Y06J7A7yU21t"},"source":["Next, create a component to train a scikit-learn model. This component does the following:\n","* Imports a CSV as a pandas DataFrame.\n","* Splits the DataFrame into train and test sets.\n","* Trains a scikit-learn model.\n","* Logs metrics from the model.\n","* Saves the model artifacts as a local `model.joblib` file."]},{"cell_type":"code","execution_count":32,"metadata":{"id":"p5JBCBKyH-NC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733662341120,"user_tz":-540,"elapsed":297,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"5a2c39c8-2399-428b-a689-9fdc637f8b0a"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-32-ea938cdaaa56>:1: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n","  @component(\n","<ipython-input-32-ea938cdaaa56>:6: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n","  def sklearn_train(\n"]}],"source":["@component(\n","    packages_to_install=[\"scikit-learn==1.2\", \"pandas\", \"joblib\", \"numpy==1.26.4\"],\n","    base_image=\"python:3.10\",\n","    output_component_file=\"sklearn_train.yaml\",\n",")\n","def sklearn_train(\n","    dataset: Input[Dataset], metrics: Output[Metrics], model: Output[Model]\n","):\n","    import pandas as pd\n","    from joblib import dump\n","    from sklearn.model_selection import train_test_split\n","    from sklearn.tree import DecisionTreeClassifier\n","\n","    df = pd.read_csv(dataset.path)\n","    labels = df.pop(\"Class\").tolist()\n","    data = df.values.tolist()\n","    x_train, x_test, y_train, y_test = train_test_split(data, labels)\n","\n","    skmodel = DecisionTreeClassifier()\n","    skmodel.fit(x_train, y_train)\n","    score = skmodel.score(x_test, y_test)\n","    print(\"accuracy is:\", score)\n","\n","    metrics.log_metric(\"accuracy\", (score * 100.0))\n","    metrics.log_metric(\"framework\", \"Scikit Learn\")\n","    metrics.log_metric(\"dataset_size\", len(df))\n","\n","    dump(skmodel, model.path + \".joblib\")\n",""]},{"cell_type":"markdown","metadata":{"id":"gaNNTFPaU7KT"},"source":["Finally, the last component  takes the trained model from the previous step, uploads the model to Vertex AI, and deploys it to an endpoint:"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"VGq5QCoyIEWJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733662344443,"user_tz":-540,"elapsed":294,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"0abf1fb4-93c1-40a2-8b5a-8a076dffe0c7"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-33-57c1c2d4c506>:1: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n","  @component(\n","<ipython-input-33-57c1c2d4c506>:6: DeprecationWarning: output_component_file parameter is deprecated and will eventually be removed. Please use `Compiler().compile()` to compile a component instead.\n","  def deploy_model(\n"]}],"source":["@component(\n","    packages_to_install=[\"google-cloud-aiplatform\"],\n","    base_image=\"python:3.10\",\n","    output_component_file=\"deploy_model.yaml\",\n",")\n","def deploy_model(\n","    model: Input[Model],\n","    project: str,\n","    region: str,\n","    vertex_endpoint: Output[Artifact],\n","    vertex_model: Output[Model],\n","):\n","    from google.cloud import aiplatform\n","\n","    aiplatform.init(project=project, location=region)\n","\n","    deployed_model = aiplatform.Model.upload(\n","        display_name=\"tracking_metadata_pipeline\",\n","        artifact_uri=model.uri.replace(\"model\", \"\"),\n","        serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-2:latest\",\n","    )\n","    endpoint = deployed_model.deploy(machine_type=\"n1-standard-4\")\n","\n","    # Save data to the output params\n","    vertex_endpoint.uri = endpoint.resource_name\n","    vertex_model.uri = deployed_model.resource_name"]},{"cell_type":"markdown","metadata":{"id":"UBXUgxgqA_GB"},"source":["### Define and compile the pipeline"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"66odBYKrIN4q","executionInfo":{"status":"ok","timestamp":1733662415561,"user_tz":-540,"elapsed":297,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["@dsl.pipeline(\n","    # Default pipeline root. You can override it when submitting the pipeline.\n","    pipeline_root=PIPELINE_ROOT,\n","    # A name for the pipeline.\n","    name=\"tracking_metadata_pipeline\",\n",")\n","def pipeline(\n","    bq_table: str,\n","    output_data_path: str,\n","    project: str,\n","    region: str,\n","):\n","    dataset_task = get_dataframe(project_id=project, bq_table=bq_table)\n","\n","    model_task = sklearn_train(dataset=dataset_task.output)\n","\n","    deploy_model(model=model_task.outputs[\"model\"], project=project, region=region)"]},{"cell_type":"markdown","metadata":{"id":"910541af051c"},"source":["The following generates a JSON file that is then used to run the pipeline:"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"o_wnT10RJ7-W","executionInfo":{"status":"ok","timestamp":1733662423091,"user_tz":-540,"elapsed":301,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"tracking_metadata_pipeline.json\")"]},{"cell_type":"markdown","metadata":{"id":"u-iTnzt3B6Z_"},"source":["### Initiate pipeline runs"]},{"cell_type":"markdown","source":["Create a pipeline run using a small version of the same dataset."],"metadata":{"id":"OpCeifpfXFfS"}},{"cell_type":"code","execution_count":40,"metadata":{"id":"ff4aee966c5f","executionInfo":{"status":"ok","timestamp":1733662521037,"user_tz":-540,"elapsed":603,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["from datetime import datetime\n","TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n","\n","run1 = pipeline_jobs.PipelineJob(\n","    display_name=\"tracking_metadata_pipeline\",\n","    template_path=\"tracking_metadata_pipeline.json\",\n","    job_id=\"tracking-metadata-pipeline-small-{}\".format(TIMESTAMP),\n","    parameter_values={\n","        \"bq_table\": \"sara-vertex-demos.beans_demo.small_dataset\",\n","        \"output_data_path\": \"data.csv\",\n","        \"project\": PROJECT_ID,\n","        \"region\": LOCATION,\n","    },\n","    enable_caching=True,\n",")"]},{"cell_type":"markdown","metadata":{"id":"555ac88a22cf"},"source":["Next, create another pipeline run using a larger version of the same dataset."]},{"cell_type":"code","execution_count":41,"metadata":{"id":"3d9fcb6a4a9e","executionInfo":{"status":"ok","timestamp":1733662561077,"user_tz":-540,"elapsed":305,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}}},"outputs":[],"source":["run2 = pipeline_jobs.PipelineJob(\n","    display_name=\"tracking_metadata_pipeline\",\n","    template_path=\"tracking_metadata_pipeline.json\",\n","    job_id=\"tracking-metadata-pipeline-large-{}\".format(TIMESTAMP),\n","    parameter_values={\n","        \"bq_table\": \"sara-vertex-demos.beans_demo.large_dataset\",\n","        \"output_data_path\": \"data.csv\",\n","        \"project\": PROJECT_ID,\n","        \"region\": LOCATION,\n","    },\n","    enable_caching=True,\n",")"]},{"cell_type":"markdown","metadata":{"id":"5670722f7668"},"source":["Finally, kick off pipeline executions for both runs. It's best to do this in two separate notebook cells so you can see the output for each run."]},{"cell_type":"code","execution_count":42,"metadata":{"id":"1f477f5565c6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733662566601,"user_tz":-540,"elapsed":1186,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"71a5aae9-865d-4f70-eb57-5318f1e6544e"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n","INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/721521243942/locations/us-central1/pipelineJobs/tracking-metadata-pipeline-small-20241208125520\n","INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n","INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/721521243942/locations/us-central1/pipelineJobs/tracking-metadata-pipeline-small-20241208125520')\n","INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n","https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/tracking-metadata-pipeline-small-20241208125520?project=721521243942\n"]}],"source":["run1.submit()"]},{"cell_type":"markdown","metadata":{"id":"6e682e41af78"},"source":["Then, kick off the second run:"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"cb263e503ced","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733662570296,"user_tz":-540,"elapsed":1081,"user":{"displayName":"Hangsik Shin","userId":"04632555686962088332"}},"outputId":"036baf20-71ae-4c33-c7e0-d67811f0eaf6"},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n","INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/721521243942/locations/us-central1/pipelineJobs/tracking-metadata-pipeline-large-20241208125520\n","INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n","INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/721521243942/locations/us-central1/pipelineJobs/tracking-metadata-pipeline-large-20241208125520')\n","INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n","https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/tracking-metadata-pipeline-large-20241208125520?project=721521243942\n"]}],"source":["run2.submit()"]},{"cell_type":"markdown","metadata":{"id":"TpV-iwP9qw9c"},"source":["## Cleaning up\n","\n","To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n","project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n","\n","If you don't want to delete the project, do the following to clean up the resources you used:\n","\n","* If you used Vertex AI Workbench notebooks to run this, stop or delete the notebook instance.\n","\n","* The pipeline runs you executed deployed endpoints in Vertex AI. Navigate to the [Google Cloud console](https://console.cloud.google.com/vertex-ai/endpoints) to delete those endpoints.\n","\n","* Delete the [Cloud Storage bucket](https://console.cloud.google.com/storage/browser/) you created.\n","\n","Alternatively, you can execute the below cell to clean up the resources used in this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f2c21373a498"},"outputs":[],"source":["# delete pipelines\n","try:\n","    run1.delete()\n","    run2.delete()\n","except Exception as e:\n","    print(e)\n","\n","# undeploy model from endpoints\n","endpoints = aiplatform.Endpoint.list(\n","    filter='display_name=\"beans-model-pipeline_endpoint\"'\n",")\n","for endpoint in endpoints:\n","    deployed_models = endpoint.list_models()\n","    for deployed_model in deployed_models:\n","        endpoint.undeploy(deployed_model_id=deployed_model.id)\n","    # delete endpoint\n","    endpoint.delete()\n","\n","# delete model\n","model_ids = aiplatform.Model.list(filter='display_name=\"beans-model-pipeline\"')\n","for model_id in model_ids:\n","    model = aiplatform.Model(model_name=model_id.resource_name)\n","    model.delete()\n","\n","# delete locally generated files\n","! rm -rf beans_deploy_component.yaml beans_model_component.yaml create_dataset.yaml mlmd_pipeline.json\n","\n","# delete cloud storage bucket\n","delete_bucket = False  # set True for deletion\n","if delete_bucket:\n","    ! gsutil rm -rf {BUCKET_URI}"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}